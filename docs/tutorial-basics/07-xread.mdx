---
sidebar_position: 7
---

# XREAD

**`XREAD`** is a Redis command used to read entries from one or more Redis Streams. It can read both historical and new entries, and it supports optional blocking — much like `tail -f` on a log file.

Think of `XREAD` as a simple, **single-consumer** reader — ideal for standalone Java apps, CLI tools, or batch jobs that don’t require distributed coordination.


## Syntax
```
XREAD [COUNT <count>] [BLOCK <milliseconds>] STREAMS <stream1> [stream2 ...] <id1> [id2 ...]
```


## Behavior
- Reads entries **starting from a specific ID**
- Can **block** and wait for new messages (`BLOCK`)
- Has **no acknowledgment** mechanism
- Does **not track read position**
- Suitable for simple, fire-and-forget scenarios


## Analogy
Think of `XREAD` like running linux command. It just reads data. Nothing is tracked. No coordination. No state.


```bash
tail -f log.txt
```


## When to use? 
| Scenario                                 | Recommended? | Notes                                          |
| ---------------------------------------- | ------------ | ---------------------------------------------- |
| Standalone Java app reading a stream     | ✅ Yes        | No need for consumer group overhead            |
| Batch job reading all stream data once   | ✅ Yes        | Simple linear read; no state tracking required |
| Real-time processing by a single process | ✅ Yes        | Use `BLOCK` for push-like behavior             |
| Multiple consumers / coordinated reading | ❌ No         | Use `XREADGROUP` + Consumer Groups instead     |
                


## Redis CLI
We’ll reuse the race:france stream created earlier for these examples.

* Read from begining of one stream
  ```
  XREAD STREAMS race:france 0
  ```
  * Reads all available entries starting from the start (ID = 0)

* Read Only New Messages (Real-time)
  ```
  XREAD BLOCK 0 STREAMS race:france $
  ```
  - __BLOCK 0__ → wait forever (like tail -f) until new data arrives
  - __$__ → start reading from new entries only (ignore past history)
  - In a separate terminal, add a new entry:
    ```
    XADD race:france * rider "Vingegaard" speed "33.5"
    ```

* Read from multiple streams
  ```
  XREAD STREAMS stream1 stream2 0 0
  ```
  * Returns up to 2 entries from mystream, starting at the beginning

* Read single entry from streams
  ```
  127.0.0.1:6379> xread count 1 streams race:france 1754326272465-15
  1) 1) "race:france"
    2) 1) 1) "1754326272465-16"
          2) 1) "rider"
              2) "Lopez"
              3) "speed"
              4) "26.6"
              5) "position"
              6) "17"
              7) "location_id"
              8) "1"
  ```
  * Redis returns only entries with IDs greater than the one you specify.
  * However this is confuse, as only items with IDs greater than the one provided are returned
  * In this exampole, Redis searches for the next ID > 1754326272465-15, and finds "1754326272465-16"


## Important Behaviors
| Scenario                                       | What happens?                                              |
| ---------------------------------------------- | ---------------------------------------------------------- |
| You use `XREAD STREAMS mystream 0`             | ✅ Reads **from the beginning** (gets all past messages)    |
| You use `XREAD STREAMS mystream <specific-id>` | ✅ Reads from **that point forward (exclusive)**            |
| You use `XREAD STREAMS mystream $`             | ❌ Skips all existing messages; waits for **new ones only** |


### Real-world implication
If the stream already has thousands of entries and you do:
```
XREAD BLOCK 0 STREAMS race:france $
```
You will not receive any past data — it only waits for new messages.

* __That’s great for:__
  * Real-time log tailing
  * Event subscribers that only care about new events
  * But not suitable if you want to process old data.


### How to consume past + future safely?
* __Step 1__: Backfill History
  ```
  XREAD COUNT 100 STREAMS race:france 0
  ```
  * Repeat until you’ve read up to the last known ID.

* __Step 2__: Switch to Real-time
  ```
  XREAD BLOCK 0 STREAMS race:france 1754326272465-99
  ```
  * Then switch to XREAD BLOCK 0 STREAMS mystream `<last-seen-id>`
  * This lets you continue from where you left off and receive new messages.


## No Consumer Groups Yet
We’re not covering consumer groups yet.

For simple apps, you don’t need to create consumers or groups — just read directly and track progress in your app (e.g. in memory, DB, file, etc.).

Remember:
* Redis Streams are append-only logs
* Messages stay in Redis until explicitly deleted (e.g. via XTRIM, XDEL, or eviction).
* Redis does not track any “read” position for you unless you use Consumer Groups.
*  So when using plain XREAD or XRANGE, you must track the last seen ID in your own code (e.g. in a DB, local file, in-memory var, etc.).


## What’s Next?
We’ll build a Spring Boot app that uses XREAD to:
* Load past messages as a one-time backfill
* Then switch to blocking read for live updates
The producer (you) will send new entries via XADD in Redis CLI.



## Spring Data Redis
Create spring boot project with following commands, this time we'll use h2 and Spring data JPA as well to process the data.
```
spring init -d=web,data-redis,data-jpa,h2,devtools,thymeleaf,lombok \
  -g com.example \
  -a demo \
  -p jar \
  --build maven \
  xread -x
```

add the following to application.yaml
```
spring:
  data:
    redis:
      host: localhost
      port: 6379
  h2:
    console:
      enabled: true
      path: /h2-console
  datasource:
    name: example
    username: sa
    password:
    generate-unique-name: false
    url: jdbc:h2:mem:testdb
    driver-class-name: org.h2.Driver
  sql:
    init:
      mode: never
  jpa:
    open-in-view: false
    show-sql: true
    generate-ddl: true
    properties:
      hibernate:
        jdbc:
          lob:
            non_contextual_creation: true
        format_sql: true
        generate_statistics: true
    hibernate:
      ddl-auto: create-drop

logging:
  level:
    root: INFO
    org:
      springframework:
        security: TRACE
        web:
          reactive:
            function:
              client: DEBUG
      hibernate:
        SQL: DEBUG
        orm:
          jdbc:
            bind: TRACE
```

RaceStreamEntity.java
```
@Entity
@Table(name = "race_stream_entry")
@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
@Builder
public class RaceStreamEntity {

    @Id
    @GeneratedValue
    @UuidGenerator
    @Setter(AccessLevel.NONE)
    private UUID id;

    @Column(name = "stream_id", unique = true)
    private String streamId; // Redis Stream ID (e.g., 1754326272465-45)

    @Column(name = "rider")
    private String rider;

    @Column(name = "speed")
    private Double speed;

    @Column(name = "position")
    private Integer position;

    @Column(name = "location_id")
    private Integer locationId;

    @Column(name = "received_at")
    private LocalDateTime receivedAt;

}
```

RaceStreamRepository.java
```java

@Repository
public interface RaceStreamRepository extends JpaRepository<RaceStreamEntity, UUID>, JpaSpecificationExecutor<RaceStreamEntity> {

    boolean existsByStreamId(String streamId);

    Optional<RaceStreamEntity> findTopByOrderByReceivedAtDesc();

}
```

RaceStreamService.java
```
public interface RaceStreamService {

    void readHistorical();

    void tailStream();
}
```

RaceStreamServiceImpl.java
```
@Slf4j
@Service
@RequiredArgsConstructor
public class RaceStreamServiceImpl implements RaceStreamService {

    private final StringRedisTemplate redisTemplate;
    private final RaceStreamRepository repository;
    private static final String STREAM_KEY = "race:france";
    private final ExecutorService executor = Executors.newSingleThreadExecutor();

    @PostConstruct
    public void autoStartConsumer() {
        executor.submit(this::tailStream);
    }

    @Scheduled(fixedRate = 10_000) // every 10 second
    public void scheduledReadHistorical() {
        System.out.println("[Scheduler] Run # Executing readHistorical()");
        readHistorical();
    }

    @Override
    public void readHistorical() {

        String lastSeenId = repository.findTopByOrderByReceivedAtDesc()
                .map(RaceStreamEntity::getStreamId)
                .orElse("0");


        StreamOperations<String, Object, Object> streamOps = redisTemplate.opsForStream();

        // Read up to 10 entries in a single call
        List<MapRecord<String, Object, Object>> records = streamOps.read(
                StreamReadOptions.empty().count(10),
                StreamOffset.create(STREAM_KEY, ReadOffset.from(lastSeenId))
        );

        if (records == null || records.isEmpty()) {
            return; // nothing to process
        }

        for (MapRecord<String, Object, Object> record : records) {
            String streamId = record.getId().getValue();

            if (!repository.existsByStreamId(streamId)) {
                RaceStreamEntity entity = toEntity(record);
                repository.save(entity);
            }
            lastSeenId = streamId; // ✅ advance pointer (if needed)
        }
        log.info("✅ Last processed streamId: {}", lastSeenId);

    }

    @Override
    public void tailStream() {
        StreamOperations<String, Object, Object> streamOps = redisTemplate.opsForStream();
        String lastSeenId = "$"; // only new messages

        while (true) {
            try {
                List<MapRecord<String, Object, Object>> records = streamOps.read(
                        StreamReadOptions.empty().block(Duration.ofSeconds(10)).count(10),
                        StreamOffset.create(STREAM_KEY, ReadOffset.from(lastSeenId))
                );

                if (records == null || records.isEmpty()) {
                    continue; // wait again
                }

                for (MapRecord<String, Object, Object> record : records) {
                    String streamId = record.getId().getValue();

                    if (!repository.existsByStreamId(streamId)) {
                        RaceStreamEntity entry = toEntity(record);
                        repository.save(entry);
                    }

                    lastSeenId = streamId;
                }

            } catch (Exception ex) {
                ex.printStackTrace(); // log and continue
            }
        }
    }

    private RaceStreamEntity toEntity(MapRecord<String, Object, Object> record) {
        Map<Object, Object> valueMap = record.getValue();
        return RaceStreamEntity.builder()
                .streamId(record.getId().getValue())
                .rider(valueMap.getOrDefault("rider", "").toString())
                .speed(Double.valueOf(valueMap.getOrDefault("speed", "0").toString()))
                .position(Integer.valueOf(valueMap.getOrDefault("position", "0").toString()))
                .locationId(Integer.valueOf(valueMap.getOrDefault("location_id", "0").toString()))
                .receivedAt(LocalDateTime.now())
                .build();
    }
}
```


from the DemoApplication.java
```java
import lombok.RequiredArgsConstructor;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.data.domain.Range;
import org.springframework.data.redis.connection.stream.MapRecord;
import org.springframework.data.redis.core.StreamOperations;
import org.springframework.data.redis.connection.Limit;
import org.springframework.data.redis.core.StringRedisTemplate;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import java.util.List;

@SpringBootApplication
@EnableScheduling
public class DemoApplication {

	public static void main(String[] args) {
		SpringApplication.run(DemoApplication.class, args);
	}

}
```

In this example, we demonstrate how to use the receivedAt field (i.e., the last processed record's stream ID) to resume reading from a Redis stream. Based on this ID, we continue querying for subsequent stream entries. Additionally, we enable blocking behavior to wait for any new incoming entries.

While this approach appears to work smoothly, there’s a critical drawback.

__Scenario:__
* You're reading 10 old records starting from ID 100-0 up to 109-0.
* While you're still processing record 105-0, a new entry 200-0 is added.
* Your logic stores 200-0 in DB and updates receivedAt = 200-0.
* Now, records 106-0 to 199-0 are skipped forever, unless you backtrack manually.



If a new stream entry arrives while we're still processing historical records, and we process and store that new entry first, our "last seen ID" gets prematurely updated to this newer ID. As a result, any unprocessed entries between the old and new IDs may be skipped permanently, and never picked up again.

To avoid this, it's better not to manage the offset ourselves. Instead, we should rely on Redis consumer groups, which are specifically designed to track the delivery state per consumer, ensuring we always resume exactly where we left off — without gaps or duplicates.

In short:
👉 Consumers should be stateless and dumb — let Redis tell us what to process next.

:::success tips
### Better approach for real-world consumers
Use Consumer Groups + XREADGROUP:
```
XGROUP CREATE mystream mygroup 0    # Create once
XREADGROUP GROUP mygroup consumerA COUNT 10 BLOCK 0 STREAMS mystream >
```
* ✅ Will receive unprocessed historical messages
* ✅ Progress tracked per group
* ✅ Avoids duplication
* ✅ Supports XACK to acknowledge processed messages

We’ll cover Consumer Groups in the next section.
:::